{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src import utils, models, metrics"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "df_movies, df_users, df_ratings = utils.read_pickles(\"../../data/ml-1m-after_eda/\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "train, test = utils.TrainTestSplitter.split_by_percent(df_ratings, 0.8,\n",
    "                                                       sort_by_datetime=True)\n",
    "true_ratings = test[\"Rating\"].values\n",
    "true_ratings"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "base_model = models.BaseModelAverage()\n",
    "base_model.fit(train)\n",
    "predicted_scores_baseline = base_model.predict(test[\"MovieID\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "predicted_scores_baseline"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepLearning predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "df = utils.dl_data_pipeline(df_movies, df_users, df_ratings)\n",
    "df = df.drop([\"UserID\", \"MovieID\"], axis=1)\n",
    "df.head(3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "train, test = utils.TrainTestSplitter.split_by_percent(df, 0.8)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "mean_users_rating = train[\"AvgUserRating\"].mean()\n",
    "mean_movies_rating = train[\"AvgMovieRating\"].mean()\n",
    "\n",
    "train[\"AvgUserRating\"] = train[\"AvgUserRating\"].fillna(mean_users_rating)\n",
    "train[\"AvgMovieRating\"] = train[\"AvgMovieRating\"].fillna(mean_movies_rating)\n",
    "\n",
    "test[\"AvgUserRating\"] = test[\"AvgUserRating\"].fillna(mean_users_rating)\n",
    "test[\"AvgMovieRating\"] = test[\"AvgMovieRating\"].fillna(mean_movies_rating)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "model = models.MovieRatingNN(112)\n",
    "model.load_state_dict(torch.load(\"../../artifacts/simple_nn.pth\"))\n",
    "model.eval()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "test_tensor = torch.tensor(test.drop([\"Rating\"], axis=1).values,\n",
    "                           dtype=torch.float32)\n",
    "predicted_scores_nn = model.forward(test_tensor).detach().numpy().flatten()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A/B Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "\n",
    "class IncrementalABTester:\n",
    "    def __init__(self):\n",
    "        self.true_ratings = np.array([])\n",
    "        self.predictions_baseline = np.array([])\n",
    "        self.predictions_nn = np.array([])\n",
    "        self.p_hist = []\n",
    "\n",
    "    def add_data(self, true_rating, prediction_baseline, prediction_nn):\n",
    "        self.true_ratings = np.append(self.true_ratings, true_rating)\n",
    "        self.predictions_baseline = np.append(self.predictions_baseline, prediction_baseline)\n",
    "        self.predictions_nn = np.append(self.predictions_nn, prediction_nn)\n",
    "\n",
    "    def mse(self, predictions):\n",
    "        return np.mean((self.true_ratings - predictions) ** 2)\n",
    "\n",
    "    def run_tests(self):\n",
    "        rmse_baseline = self.mse(self.predictions_baseline)\n",
    "        rmse_nn = self.mse(self.predictions_nn)\n",
    "        \n",
    "        # Conduct a paired t-test if we have enough data\n",
    "        if len(self.true_ratings) > 1:\n",
    "            _, p_value = stats.ttest_rel(self.predictions_baseline, self.predictions_nn)\n",
    "        else:\n",
    "            p_value = np.nan  # Not enough data to test\n",
    "        self.p_hist.append(p_value)\n",
    "\n",
    "        results = {\n",
    "            'rmse_baseline': round(rmse_baseline,3),\n",
    "            'rmse_nn': round(rmse_nn,3),\n",
    "            'p_value': round(p_value,3)\n",
    "        }\n",
    "        \n",
    "        return results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "tester = IncrementalABTester()\n",
    "\n",
    "for true, baseline, nn in zip(true_ratings, predicted_scores_baseline,\n",
    "                              predicted_scores_nn):\n",
    "    tester.add_data(true, baseline, nn)\n",
    "    if len(tester.true_ratings) % 10 == 0:  # Periodic testing, e.g., every 10 record\n",
    "        results = tester.run_tests()\n",
    "    if len(tester.true_ratings) % 50000 == 0:\n",
    "        print(f\"After {len(tester.true_ratings)} records: \", results)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(tester.p_hist)\n",
    "plt.xlim(100, 300)\n",
    "plt.xlabel(\"Records\")\n",
    "plt.ylabel(\"p_value\")\n",
    "plt.grid(0.3)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "source": [
    "# Calculate errors\n",
    "errors_baseline = abs(true_ratings - predicted_scores_baseline)\n",
    "errors_nn = abs(true_ratings - predicted_scores_nn)\n",
    "\n",
    "# Calculate mean errors\n",
    "mean_abs_error_baseline = np.mean(errors_baseline)\n",
    "mean_abs_error_nn = np.mean(errors_nn)\n",
    "\n",
    "# Plotting histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(errors_baseline, bins=100, alpha=0.5, label='Baseline Errors', color='blue')\n",
    "plt.hist(errors_nn, bins=100, alpha=0.5, label='NN Errors', color='red')\n",
    "\n",
    "# Adding mean lines\n",
    "plt.axvline(mean_abs_error_baseline, color='blue', linestyle='dashed', linewidth=2, label=f'Mean Baseline Error: {mean_error_baseline:.2f}')\n",
    "plt.axvline(mean_abs_error_nn, color='red', linestyle='dashed', linewidth=2, label=f'Mean NN Error: {mean_error_nn:.2f}')\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel('Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Prediction Errors')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucu-recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
